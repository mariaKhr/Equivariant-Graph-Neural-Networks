{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f6a5090"
      },
      "source": [
        "# Simple Impementation of E(n) Equivariant Graph Neural Networks\n",
        "\n",
        "Original paper https://arxiv.org/pdf/2102.09844.pdf by Victor Garcia Satorras, Emiel Hoogeboom, Max Welling"
      ],
      "id": "6f6a5090"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bU4ixrOJCg1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "4bU4ixrOJCg1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cb08a10"
      },
      "source": [
        "# Load QM9 Dataset"
      ],
      "id": "8cb08a10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae30de9d",
        "outputId": "ad81a85e-2f3b-4046-ff99-3a9d5c1d5675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sergk\\Desktop\\simple-equivariant-gnn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'simple-equivariant-gnn'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/senya-ashukha/simple-equivariant-gnn.git\n",
        "%cd simple-equivariant-gnn"
      ],
      "id": "ae30de9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "859f981c"
      },
      "outputs": [],
      "source": [
        "# QM9 is a dataset for Molecular Property Predictions http://quantum-machine.org/datasets/\n",
        "# We will predict Highest occupied molecular orbital energy \n",
        "# https://en.wikipedia.org/wiki/HOMO_and_LUMO\n",
        "# We use data loaders from the official repo\n",
        "\n",
        "from qm9.data_utils import get_data, BatchGraph\n",
        "train_loader, val_loader, test_loader, charge_scale = get_data(num_workers=0)"
      ],
      "id": "859f981c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05e20004"
      },
      "source": [
        "# Graph Representation"
      ],
      "id": "05e20004"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0acbcc0",
        "outputId": "c8784323-bb0b-44e6-f2e9-0d1228de0d64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "In the batch: num_graphs 96 num_nodes 1733\n",
              "> .h \t\t a tensor of nodes representations \t\tshape 1733 x 15\n",
              "> .x \t\t a tensor of nodes positions  \t\t\tshape 1733 x 3\n",
              "> .edges \t a tensor of edges, a fully connected graph \tshape 30468 x 2\n",
              "> .batch  \t a tensor of graph_ids for each node \t\ttensor([ 0,  0,  0,  ..., 95, 95, 95])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = BatchGraph(iter(train_loader).next(), False, charge_scale)\n",
        "batch"
      ],
      "id": "d0acbcc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "784c0726"
      },
      "source": [
        "# Define Equivariant Graph Convs  & GNN"
      ],
      "id": "784c0726"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76e5e05f"
      },
      "outputs": [],
      "source": [
        "def index_sum(agg_size, source, idx, cuda):\n",
        "    \"\"\"\n",
        "        source is N x hid_dim [float]\n",
        "        idx    is N           [int]\n",
        "        \n",
        "        Sums the rows source[.] with the same idx[.];\n",
        "    \"\"\"\n",
        "    tmp = torch.zeros((agg_size, source.shape[1]))\n",
        "    tmp = tmp.cuda() if cuda else tmp\n",
        "    res = torch.index_add(tmp, 0, idx, source)\n",
        "    return res"
      ],
      "id": "76e5e05f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d5d55db"
      },
      "outputs": [],
      "source": [
        "class ConvEGNN(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.cuda = cuda\n",
        "        \n",
        "        self.f_e = nn.Sequential(nn.Linear(in_dim * 2 + 1, hid_dim), \n",
        "                                 nn.SiLU(),\n",
        "                                 nn.Linear(hid_dim, hid_dim), \n",
        "                                 nn.SiLU())\n",
        "        \n",
        "        self.f_inf = nn.Sequential(nn.Linear(hid_dim, 1), \n",
        "                                   nn.Sigmoid()) \n",
        "        \n",
        "        self.f_h = nn.Sequential(nn.Linear(hid_dim + in_dim, hid_dim), \n",
        "                                 nn.SiLU(),\n",
        "                                 nn.Linear(hid_dim, hid_dim))\n",
        "    \n",
        "    def forward(self, b):\n",
        "        e_st, e_end = b.edges[:,0], b.edges[:,1]\n",
        "        dists = torch.norm(b.x[e_st] - b.x[e_end], dim=1).reshape(-1, 1)\n",
        "        \n",
        "        tmp = torch.hstack([b.h[e_st], b.h[e_end], dists])\n",
        "        m_ij = self.f_e(tmp)\n",
        "        e_ij = self.f_inf(m_ij)\n",
        "        \n",
        "        m_i = index_sum(b.h.shape[0], e_ij * m_ij, b.edges[:,0], self.cuda)\n",
        "        b.h = b.h + self.f_h(torch.hstack([b.h, m_i]))\n",
        "\n",
        "        return b"
      ],
      "id": "4d5d55db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10aad7c4"
      },
      "outputs": [],
      "source": [
        "class NetEGNN(nn.Module):\n",
        "    def __init__(self, in_dim=15, hid_dim=128, out_dim=1, n_layers=7, cuda=True):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "\n",
        "        self.emb = nn.Linear(in_dim, hid_dim) \n",
        "        self.gnn = [ConvEGNN(hid_dim, hid_dim, cuda=cuda) for i in range(n_layers)]\n",
        "        self.gnn = nn.Sequential(*self.gnn)\n",
        "        \n",
        "        self.pre_mlp = nn.Sequential(\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, hid_dim))\n",
        "        \n",
        "        self.post_mlp = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(hid_dim, hid_dim), nn.SiLU(),\n",
        "            nn.Linear(hid_dim, out_dim))\n",
        "\n",
        "        if cuda: self.cuda()\n",
        "        self.cuda = cuda\n",
        "    \n",
        "    def forward(self, b):\n",
        "        b.h = self.emb(b.h)\n",
        "        \n",
        "        b = self.gnn(b)\n",
        "        h_nodes = self.pre_mlp(b.h)\n",
        "        h_graph = index_sum(b.nG, h_nodes, b.batch, self.cuda) \n",
        "        \n",
        "        out = self.post_mlp(h_graph)\n",
        "        return out"
      ],
      "id": "10aad7c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7f4cef6"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "cuda = True\n",
        "\n",
        "model = NetEGNN(n_layers=7, cuda=cuda)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-16)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, verbose=False)"
      ],
      "id": "b7f4cef6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e5d6b1c"
      },
      "source": [
        "# Training"
      ],
      "id": "4e5d6b1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "de3613c9",
        "outputId": "8df3c633-181a-4fc8-9b57-a0337175049e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> start training\n",
            "> epoch 000: train 375.076 val 290.748 test 290.828 (209.0 sec)\n",
            "> epoch 001: train 280.371 val 243.631 test 239.648 (207.6 sec)\n",
            "> epoch 002: train 226.147 val 209.887 test 207.483 (208.5 sec)\n",
            "> epoch 003: train 198.120 val 203.409 test 201.595 (208.5 sec)\n",
            "> epoch 004: train 178.293 val 168.687 test 166.331 (208.8 sec)\n",
            "> epoch 005: train 165.013 val 163.209 test 163.494 (209.3 sec)\n",
            "> epoch 006: train 152.334 val 134.529 test 134.883 (209.5 sec)\n",
            "> epoch 007: train 142.115 val 126.247 test 125.664 (209.0 sec)\n",
            "> epoch 008: train 135.106 val 116.740 test 116.071 (209.1 sec)\n",
            "> epoch 009: train 128.091 val 120.168 test 118.685 (209.5 sec)\n",
            "> epoch 010: train 122.549 val 124.728 test 123.993 (209.2 sec)\n",
            "> epoch 011: train 118.251 val 120.148 test 118.278 (209.1 sec)\n",
            "> epoch 012: train 112.912 val 104.347 test 103.627 (209.5 sec)\n",
            "> epoch 013: train 108.226 val 100.332 test 100.073 (209.1 sec)\n",
            "> epoch 014: train 104.159 val 98.225 test 96.702 (209.3 sec)\n",
            "> epoch 015: train 101.684 val 98.306 test 97.454 (210.1 sec)\n",
            "> epoch 016: train 97.979 val 91.601 test 91.659 (210.4 sec)\n",
            "> epoch 017: train 96.024 val 98.556 test 99.569 (208.8 sec)\n",
            "> epoch 018: train 93.393 val 88.719 test 87.575 (209.6 sec)\n",
            "> epoch 019: train 90.874 val 91.308 test 90.443 (209.1 sec)\n",
            "> epoch 020: train 89.284 val 88.739 test 88.729 (209.3 sec)\n",
            "> epoch 021: train 87.370 val 91.099 test 92.486 (208.2 sec)\n",
            "> epoch 022: train 85.418 val 80.582 test 80.206 (208.8 sec)\n",
            "> epoch 023: train 84.307 val 84.421 test 84.315 (207.6 sec)\n",
            "> epoch 024: train 83.021 val 77.183 test 77.554 (207.9 sec)\n",
            "> epoch 025: train 80.769 val 80.048 test 80.184 (207.7 sec)\n",
            "> epoch 026: train 79.838 val 75.306 test 74.827 (207.5 sec)\n",
            "> epoch 027: train 79.167 val 75.840 test 75.679 (208.3 sec)\n",
            "> epoch 028: train 77.902 val 74.323 test 74.014 (207.7 sec)\n",
            "> epoch 029: train 76.848 val 75.238 test 74.799 (208.0 sec)\n",
            "> epoch 030: train 75.095 val 75.479 test 75.762 (207.4 sec)\n",
            "> epoch 031: train 74.361 val 78.377 test 78.776 (208.2 sec)\n",
            "> epoch 032: train 73.622 val 77.301 test 77.864 (208.6 sec)\n",
            "> epoch 033: train 72.789 val 75.334 test 75.619 (208.4 sec)\n",
            "> epoch 034: train 72.176 val 69.462 test 70.041 (207.3 sec)\n",
            "> epoch 035: train 71.056 val 69.176 test 69.523 (208.3 sec)\n",
            "> epoch 036: train 70.000 val 65.768 test 65.518 (208.4 sec)\n",
            "> epoch 037: train 69.379 val 72.225 test 72.262 (207.4 sec)\n",
            "> epoch 038: train 69.059 val 64.427 test 65.085 (208.1 sec)\n",
            "> epoch 039: train 67.180 val 68.152 test 68.525 (207.9 sec)\n",
            "> epoch 040: train 66.969 val 65.647 test 64.957 (207.5 sec)\n",
            "> epoch 041: train 66.560 val 69.631 test 69.606 (207.6 sec)\n",
            "> epoch 042: train 65.875 val 64.292 test 64.598 (208.0 sec)\n",
            "> epoch 043: train 66.177 val 61.444 test 61.939 (208.2 sec)\n",
            "> epoch 044: train 65.067 val 65.605 test 66.261 (208.0 sec)\n",
            "> epoch 045: train 64.347 val 69.905 test 70.638 (208.4 sec)\n",
            "> epoch 046: train 63.324 val 64.563 test 64.205 (207.6 sec)\n",
            "> epoch 047: train 62.424 val 61.586 test 62.042 (209.2 sec)\n",
            "> epoch 048: train 61.955 val 60.267 test 60.186 (207.4 sec)\n",
            "> epoch 049: train 61.853 val 61.690 test 62.066 (208.5 sec)\n",
            "> epoch 050: train 60.586 val 61.672 test 62.083 (208.4 sec)\n",
            "> epoch 051: train 60.444 val 61.097 test 61.430 (208.8 sec)\n",
            "> epoch 052: train 60.224 val 56.757 test 57.309 (208.2 sec)\n",
            "> epoch 053: train 59.456 val 59.787 test 59.525 (207.8 sec)\n",
            "> epoch 054: train 58.487 val 57.660 test 58.289 (208.6 sec)\n",
            "> epoch 055: train 58.544 val 57.398 test 57.765 (208.0 sec)\n",
            "> epoch 056: train 58.021 val 61.879 test 61.514 (208.1 sec)\n",
            "> epoch 057: train 57.309 val 56.646 test 56.799 (208.6 sec)\n",
            "> epoch 058: train 56.906 val 56.340 test 57.125 (208.2 sec)\n",
            "> epoch 059: train 56.141 val 58.336 test 58.462 (207.8 sec)\n",
            "> epoch 060: train 56.316 val 60.569 test 60.358 (207.7 sec)\n",
            "> epoch 061: train 55.968 val 55.963 test 57.192 (208.6 sec)\n",
            "> epoch 062: train 55.330 val 57.935 test 57.976 (208.7 sec)\n",
            "> epoch 063: train 54.688 val 56.503 test 56.095 (207.4 sec)\n",
            "> epoch 064: train 53.946 val 58.375 test 58.086 (207.6 sec)\n",
            "> epoch 065: train 53.449 val 54.417 test 54.569 (208.1 sec)\n",
            "> epoch 066: train 53.322 val 52.753 test 53.189 (208.3 sec)\n",
            "> epoch 067: train 52.987 val 53.753 test 54.552 (207.3 sec)\n",
            "> epoch 068: train 52.108 val 51.972 test 52.586 (20744.4 sec)\n",
            "> epoch 069: train 51.619 val 54.751 test 55.048 (208.5 sec)\n",
            "> epoch 070: train 51.331 val 53.991 test 54.380 (207.4 sec)\n",
            "> epoch 071: train 51.266 val 54.194 test 54.106 (207.8 sec)\n",
            "> epoch 072: train 50.255 val 54.340 test 54.481 (206.4 sec)\n",
            "> epoch 073: train 49.998 val 52.806 test 53.148 (207.5 sec)\n",
            "> epoch 074: train 49.670 val 52.915 test 53.356 (208.1 sec)\n",
            "> epoch 075: train 49.162 val 55.233 test 55.630 (206.9 sec)\n",
            "> epoch 076: train 48.617 val 53.951 test 54.588 (207.3 sec)\n",
            "> epoch 077: train 48.173 val 53.485 test 54.313 (206.6 sec)\n",
            "> epoch 078: train 47.907 val 50.311 test 50.771 (206.7 sec)\n",
            "> epoch 079: train 47.369 val 52.140 test 52.383 (207.3 sec)\n",
            "> epoch 080: train 46.755 val 52.337 test 52.592 (206.7 sec)\n",
            "> epoch 081: train 46.486 val 51.903 test 51.972 (207.3 sec)\n",
            "> epoch 082: train 46.178 val 49.719 test 50.307 (207.4 sec)\n",
            "> epoch 083: train 45.711 val 48.837 test 49.591 (206.6 sec)\n",
            "> epoch 084: train 44.990 val 49.748 test 50.203 (206.9 sec)\n",
            "> epoch 085: train 44.438 val 48.470 test 48.986 (207.2 sec)\n",
            "> epoch 086: train 44.683 val 49.766 test 50.196 (208.0 sec)\n",
            "> epoch 087: train 44.081 val 48.620 test 49.358 (207.9 sec)\n",
            "> epoch 088: train 43.798 val 51.438 test 51.660 (207.5 sec)\n",
            "> epoch 089: train 42.968 val 48.424 test 48.682 (208.1 sec)\n",
            "> epoch 090: train 42.733 val 49.258 test 49.215 (208.2 sec)\n",
            "> epoch 091: train 42.163 val 49.929 test 49.975 (207.4 sec)\n",
            "> epoch 092: train 41.834 val 50.363 test 50.538 (208.5 sec)\n",
            "> epoch 093: train 42.089 val 48.460 test 48.313 (207.5 sec)\n",
            "> epoch 094: train 41.635 val 47.222 test 47.880 (207.2 sec)\n",
            "> epoch 095: train 40.568 val 48.556 test 48.456 (207.0 sec)\n",
            "> epoch 096: train 40.459 val 47.070 test 47.960 (206.1 sec)\n",
            "> epoch 097: train 40.097 val 47.884 test 48.917 (206.8 sec)\n",
            "> epoch 098: train 39.624 val 46.370 test 46.641 (206.2 sec)\n",
            "> epoch 099: train 39.440 val 45.526 test 46.211 (206.4 sec)\n",
            "> epoch 100: train 39.174 val 47.321 test 47.516 (206.0 sec)\n",
            "> epoch 101: train 38.984 val 45.644 test 46.265 (207.2 sec)\n",
            "> epoch 102: train 38.226 val 46.641 test 46.728 (206.7 sec)\n",
            "> epoch 103: train 37.866 val 45.522 test 45.483 (207.1 sec)\n",
            "> epoch 104: train 37.710 val 44.458 test 44.836 (206.6 sec)\n",
            "> epoch 105: train 37.390 val 45.844 test 46.088 (206.6 sec)\n",
            "> epoch 106: train 37.043 val 44.790 test 45.428 (207.0 sec)\n",
            "> epoch 107: train 36.861 val 45.376 test 46.538 (206.5 sec)\n",
            "> epoch 108: train 36.169 val 43.299 test 43.865 (207.3 sec)\n",
            "> epoch 109: train 35.762 val 44.752 test 45.083 (207.4 sec)\n",
            "> epoch 110: train 35.544 val 44.549 test 45.674 (206.9 sec)\n",
            "> epoch 111: train 35.298 val 42.714 test 43.626 (206.1 sec)\n",
            "> epoch 112: train 35.222 val 43.962 test 44.046 (206.3 sec)\n",
            "> epoch 113: train 34.794 val 44.498 test 44.642 (206.5 sec)\n",
            "> epoch 114: train 34.334 val 43.334 test 43.962 (206.9 sec)\n",
            "> epoch 115: train 33.858 val 44.834 test 44.878 (206.1 sec)\n",
            "> epoch 116: train 33.654 val 45.641 test 46.071 (205.9 sec)\n",
            "> epoch 117: train 33.322 val 42.653 test 43.670 (207.6 sec)\n",
            "> epoch 118: train 32.873 val 43.240 test 43.536 (207.0 sec)\n",
            "> epoch 119: train 32.932 val 43.049 test 43.504 (206.6 sec)\n",
            "> epoch 120: train 32.375 val 43.872 test 43.991 (206.6 sec)\n",
            "> epoch 121: train 32.171 val 41.735 test 42.097 (206.3 sec)\n",
            "> epoch 122: train 31.711 val 42.696 test 42.842 (206.2 sec)\n",
            "> epoch 123: train 31.362 val 41.374 test 41.313 (206.8 sec)\n",
            "> epoch 124: train 31.255 val 42.907 test 43.135 (206.6 sec)\n",
            "> epoch 125: train 31.051 val 41.936 test 42.433 (206.7 sec)\n",
            "> epoch 126: train 30.537 val 43.389 test 43.516 (206.2 sec)\n",
            "> epoch 127: train 30.566 val 40.516 test 40.638 (207.2 sec)\n",
            "> epoch 128: train 30.151 val 43.038 test 43.283 (206.9 sec)\n",
            "> epoch 129: train 29.785 val 40.833 test 41.120 (206.5 sec)\n",
            "> epoch 130: train 29.523 val 40.831 test 41.143 (206.5 sec)\n",
            "> epoch 131: train 29.211 val 40.088 test 40.573 (206.0 sec)\n",
            "> epoch 132: train 28.805 val 40.085 test 40.573 (206.0 sec)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> epoch 133: train 28.542 val 41.007 test 41.434 (205.3 sec)\n",
            "> epoch 134: train 28.439 val 40.133 test 40.614 (206.4 sec)\n",
            "> epoch 135: train 28.052 val 41.220 test 42.058 (206.4 sec)\n",
            "> epoch 136: train 28.013 val 40.254 test 40.628 (206.5 sec)\n",
            "> epoch 137: train 27.546 val 40.672 test 41.168 (206.8 sec)\n",
            "> epoch 138: train 27.234 val 39.695 test 39.773 (206.6 sec)\n",
            "> epoch 139: train 27.143 val 39.068 test 39.803 (206.8 sec)\n",
            "> epoch 140: train 26.820 val 39.781 test 40.294 (206.3 sec)\n",
            "> epoch 141: train 26.661 val 40.505 test 40.806 (207.6 sec)\n",
            "> epoch 142: train 26.438 val 39.987 test 40.541 (206.6 sec)\n",
            "> epoch 143: train 26.241 val 39.320 test 40.172 (206.7 sec)\n",
            "> epoch 144: train 25.896 val 38.840 test 39.355 (206.4 sec)\n",
            "> epoch 145: train 25.812 val 39.397 test 40.125 (206.7 sec)\n",
            "> epoch 146: train 25.389 val 39.450 test 39.938 (207.4 sec)\n",
            "> epoch 147: train 25.232 val 38.391 test 38.997 (207.2 sec)\n",
            "> epoch 148: train 25.061 val 38.299 test 38.898 (207.1 sec)\n",
            "> epoch 149: train 24.905 val 38.384 test 38.935 (207.3 sec)\n",
            "> epoch 150: train 24.771 val 38.540 test 38.968 (21859.7 sec)\n",
            "> epoch 151: train 24.403 val 38.283 test 39.077 (208.5 sec)\n",
            "> epoch 152: train 24.261 val 38.244 test 38.676 (209.4 sec)\n",
            "> epoch 153: train 24.010 val 38.582 test 39.129 (209.3 sec)\n",
            "> epoch 154: train 23.667 val 38.371 test 39.065 (209.4 sec)\n",
            "> epoch 155: train 23.771 val 38.630 test 39.378 (208.2 sec)\n",
            "> epoch 156: train 23.473 val 38.038 test 38.535 (209.0 sec)\n",
            "> epoch 157: train 23.424 val 37.815 test 38.496 (209.0 sec)\n",
            "> epoch 158: train 23.067 val 37.601 test 38.425 (209.3 sec)\n",
            "> epoch 159: train 22.847 val 38.288 test 38.819 (209.4 sec)\n",
            "> epoch 160: train 22.767 val 38.256 test 38.660 (209.4 sec)\n",
            "> epoch 161: train 22.524 val 37.592 test 38.209 (209.4 sec)\n",
            "> epoch 162: train 22.283 val 37.990 test 38.539 (208.9 sec)\n",
            "> epoch 163: train 22.260 val 37.513 test 38.169 (209.1 sec)\n",
            "> epoch 164: train 22.099 val 37.773 test 38.386 (209.5 sec)\n",
            "> epoch 165: train 22.019 val 37.976 test 38.628 (208.9 sec)\n",
            "> epoch 166: train 21.901 val 38.182 test 38.594 (208.7 sec)\n",
            "> epoch 167: train 21.756 val 37.682 test 38.288 (209.9 sec)\n",
            "> epoch 168: train 21.444 val 37.123 test 37.831 (209.4 sec)\n",
            "> epoch 169: train 21.371 val 37.269 test 37.908 (210.5 sec)\n",
            "> epoch 170: train 21.325 val 37.972 test 38.537 (209.1 sec)\n",
            "> epoch 171: train 21.237 val 37.411 test 38.097 (209.8 sec)\n",
            "> epoch 172: train 20.946 val 37.422 test 38.016 (209.1 sec)\n",
            "> epoch 173: train 20.883 val 37.062 test 37.711 (208.6 sec)\n",
            "> epoch 174: train 20.705 val 37.187 test 37.764 (209.2 sec)\n",
            "> epoch 175: train 20.610 val 36.988 test 37.589 (208.3 sec)\n",
            "> epoch 176: train 20.562 val 37.487 test 38.045 (209.9 sec)\n",
            "> epoch 177: train 20.477 val 37.180 test 37.626 (209.3 sec)\n",
            "> epoch 178: train 20.413 val 37.099 test 37.591 (209.6 sec)\n",
            "> epoch 179: train 20.412 val 36.996 test 37.515 (209.6 sec)\n",
            "> epoch 180: train 20.249 val 37.186 test 37.679 (209.2 sec)\n",
            "> epoch 181: train 20.067 val 37.237 test 37.723 (209.7 sec)\n",
            "> epoch 182: train 20.134 val 37.053 test 37.622 (210.1 sec)\n",
            "> epoch 183: train 20.118 val 37.299 test 37.830 (209.6 sec)\n",
            "> epoch 184: train 19.973 val 37.121 test 37.677 (210.2 sec)\n",
            "> epoch 185: train 20.081 val 36.990 test 37.562 (209.4 sec)\n",
            "> epoch 186: train 19.833 val 36.875 test 37.452 (209.1 sec)\n",
            "> epoch 187: train 19.942 val 37.139 test 37.700 (210.1 sec)\n",
            "> epoch 188: train 19.802 val 36.909 test 37.502 (209.5 sec)\n",
            "> epoch 189: train 19.708 val 37.061 test 37.646 (209.1 sec)\n",
            "> epoch 190: train 19.731 val 37.009 test 37.592 (209.8 sec)\n",
            "> epoch 191: train 19.630 val 37.106 test 37.630 (209.5 sec)\n",
            "> epoch 192: train 19.598 val 37.081 test 37.614 (208.8 sec)\n",
            "> epoch 193: train 19.587 val 37.010 test 37.566 (208.4 sec)\n",
            "> epoch 194: train 19.633 val 36.917 test 37.474 (209.4 sec)\n",
            "> epoch 195: train 19.587 val 37.027 test 37.562 (210.3 sec)\n",
            "> epoch 196: train 19.590 val 36.936 test 37.497 (211.3 sec)\n",
            "> epoch 197: train 19.519 val 36.957 test 37.516 (209.0 sec)\n",
            "> epoch 198: train 19.625 val 36.962 test 37.522 (207.5 sec)\n",
            "> epoch 199: train 19.518 val 36.953 test 37.513 (208.7 sec)\n"
          ]
        }
      ],
      "source": [
        "print('> start training')\n",
        "\n",
        "tr_ys = train_loader.dataset.data['homo'] \n",
        "me, mad = torch.mean(tr_ys), torch.mean(torch.abs(tr_ys - torch.mean(tr_ys)))\n",
        "\n",
        "if cuda:\n",
        "    me = me.cuda()\n",
        "    mad = mad.cuda()\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "test_loss = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('> epoch %s:' % str(epoch).zfill(3), end=' ', flush=True) \n",
        "    start = time.time()\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_val_loss = []\n",
        "    batch_test_loss = []\n",
        "\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = BatchGraph(batch, cuda, charge_scale)\n",
        "        \n",
        "        out = model(batch).reshape(-1)\n",
        "        loss =  F.l1_loss(out,  (batch.y-me)/mad)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            loss =  F.l1_loss(out*mad+me, batch.y)\n",
        "\n",
        "        batch_train_loss += [float(loss.data.cpu().numpy())]  \n",
        "        \n",
        "    train_loss += [np.mean(batch_train_loss)/0.001]\n",
        "    \n",
        "    print('train %.3f' % train_loss[-1], end=' ', flush=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for batch in val_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_val_loss += [np.mean(loss)]\n",
        "            \n",
        "        val_loss += [np.mean(batch_val_loss)/0.001]\n",
        "        \n",
        "        print('val %.3f' % val_loss[-1], end=' ', flush=True)\n",
        "        \n",
        "        for batch in test_loader:\n",
        "            batch = BatchGraph(batch, cuda, charge_scale)\n",
        "            out = model(batch).reshape(-1)\n",
        "            loss = F.l1_loss(out*mad+me, batch.y).data.cpu().numpy()\n",
        "            batch_test_loss += [np.mean(loss)]\n",
        "\n",
        "        test_loss += [np.mean(batch_test_loss)/0.001]\n",
        "        \n",
        "    end = time.time()\n",
        "\n",
        "    print('test %.3f (%.1f sec)' % (test_loss[-1], end-start), flush=True)\n",
        "    lr_scheduler.step()"
      ],
      "id": "de3613c9"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}